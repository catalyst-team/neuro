{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Neuro UNet Demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssktotoro/neuro/blob/tutorial_branch/Neuro%20UNet%20Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZOKa-kgP_L-"
      },
      "source": [
        "# Neuro UNet/ MeshnetTutorial\n",
        "\n",
        "Authors: [Kevin Wang] (), [Alex Fedorov] (), [Sergey Kolesnikov](https://github.com/Scitator)\n",
        "\n",
        "[![Catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)](https://github.com/catalyst-team/catalyst)\n",
        "\n",
        "### Colab setup\n",
        "\n",
        "First of all, do not forget to change the runtime type to GPU. <br/>\n",
        "To do so click `Runtime` -> `Change runtime type` -> Select `\\\"Python 3\\\"` and `\\\"GPU\\\"` -> click `Save`. <br/>\n",
        "After that you can click `Runtime` -> `Run all` and watch the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8hXadxdP_L-"
      },
      "source": [
        "## Requirements\n",
        "\n",
        "Download and install the latest versions of catalyst and other libraries required for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T0L6CqRQ-23",
        "outputId": "c6e99bd3-4d5a-4a36-b9de-270d3967aa85"
      },
      "source": [
        "%%bash \n",
        "git clone https://github.com/ssktotoro/neuro.git -b tutorial_branch\n",
        "git pull\n",
        "pip install -r neuro/requirements/requirements.txt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting alchemy==20.4\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/d0/29085429e2f6203ee206a4aa93cb20cdafbdc2aa649d7b20de24eeb7fb69/alchemy-20.4-py2.py3-none-any.whl\n",
            "Collecting catalyst==20.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/1f/7c0591a256990e146b377c282f17e2cd2717b25ac7e489c97dc972ed7248/catalyst-20.10.1-py2.py3-none-any.whl (475kB)\n",
            "Collecting reaction==20.2\n",
            "  Downloading https://files.pythonhosted.org/packages/75/9b/c549eb02e2b5caf8e2dcfb6386fa82645ffaaf2e7fc3c6d682f0591d8187/reaction-20.2-py2.py3-none-any.whl\n",
            "Collecting osfclient\n",
            "  Downloading https://files.pythonhosted.org/packages/a8/7a/8d6fe30d424329ced46a738faaea4150efb8eee656599b88a791cf7ad07e/osfclient-0.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from -r neuro/requirements/requirements.txt (line 5)) (3.0.2)\n",
            "Collecting requests==2.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (20.9)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (5.5.0)\n",
            "Requirement already satisfied: tensorboard>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: pandas>=0.22 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (3.2.2)\n",
            "Collecting GitPython>=3.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "Collecting deprecation\n",
            "  Downloading https://files.pythonhosted.org/packages/02/c3/253a89ee03fc9b9682f1541728eb66db7db22148cd94f89ab22528cd1e1b/deprecation-2.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (4.4.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.8.0+cu101)\n",
            "Collecting tensorboardX\n",
            "  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "Collecting aio-pika==6.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/49/bd/261a61a2d7d4c8f3911e39d4cc2f13162cf5db33af16df3ca8121c16710c/aio_pika-6.1.1-py3-none-any.whl (40kB)\n",
            "Collecting fastapi[all]<1.0.0,>=0.38.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/33/1b643f650688ad368983bbaf3b0658438038ea84d775dd37393d826c3833/fastapi-0.63.0-py3-none-any.whl (50kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from osfclient->-r neuro/requirements/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->alchemy==20.4->-r neuro/requirements/requirements.txt (line 1)) (3.0.4)\n",
            "Collecting idna<2.9,>=2.5\n",
            "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->alchemy==20.4->-r neuro/requirements/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->alchemy==20.4->-r neuro/requirements/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (5.0.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (54.1.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.32.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.4.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.27.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.10.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.1.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (3.7.4.3)\n",
            "Collecting aiormq~=2.3\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/ff/2cebc70ae7a96bda5048ad9ff7b2ab91c801cec9c908bddbec60de989b13/aiormq-2.9.1-py3-none-any.whl\n",
            "Collecting yarl\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "Collecting starlette==0.13.6\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/a4/c9e228d7d47044ce4c83ba002f28ff479e542455f0499198a3f77c94f564/starlette-0.13.6-py3-none-any.whl (59kB)\n",
            "Collecting pydantic<2.0.0,>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/a3/0ffdb6c63f45f10d19b8e8b32670b22ed089cafb29732f6bf8ce518821fb/pydantic-1.8.1-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "Requirement already satisfied: itsdangerous<2.0.0,>=1.1.0; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from fastapi[all]<1.0.0,>=0.38.1->reaction==20.2->-r neuro/requirements/requirements.txt (line 3)) (1.1.0)\n",
            "Collecting async_exit_stack<2.0.0,>=1.0.1; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/77/ae/f5baabf02fe19c4015b0417e72e36220fb95ed3e7cf17e8a85004b964709/async_exit_stack-1.0.1-py3-none-any.whl\n",
            "Collecting orjson<4.0.0,>=3.2.1; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/39/c3/56c4732c1d43e2d72749ea1acab7288302972e29fb3a778fdb7b804053f8/orjson-3.5.1-cp37-cp37m-manylinux2014_x86_64.whl (233kB)\n",
            "Requirement already satisfied: jinja2<3.0.0,>=2.11.2; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from fastapi[all]<1.0.0,>=0.38.1->reaction==20.2->-r neuro/requirements/requirements.txt (line 3)) (2.11.3)\n",
            "Collecting email_validator<2.0.0,>=1.1.1; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/58/f0/39459bb868ddb4e96ee3f8b1386deeceb1bf4e53de8c18a4afdf5801f24c/email_validator-1.1.2-py2.py3-none-any.whl\n",
            "Collecting aiofiles<0.6.0,>=0.5.0; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/2b/078a9771ae4b67e36b0c2a973df845260833a4eb088b81c84b738509b4c4/aiofiles-0.5.0-py3-none-any.whl\n",
            "Collecting python-multipart<0.0.6,>=0.0.5; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/40/a933ac570bf7aad12a298fc53458115cc74053474a72fbb8201d7dc06d3d/python-multipart-0.0.5.tar.gz\n",
            "Requirement already satisfied: async_generator<2.0.0,>=1.10; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from fastapi[all]<1.0.0,>=0.38.1->reaction==20.2->-r neuro/requirements/requirements.txt (line 3)) (1.10)\n",
            "Collecting ujson<4.0.0,>=3.0.0; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/15/f7d7f9d6b7db49f26aa9bcdd84e4714631c4a2bc21d0522c603492927055/ujson-3.2.0-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "Collecting uvicorn[standard]<0.14.0,>=0.12.0; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/de/953f0289508b1b92debdf0a6822d9b88ffb0c6ad471d709cf639a2c8a176/uvicorn-0.13.4-py3-none-any.whl (46kB)\n",
            "Collecting graphene<3.0.0,>=2.1.8; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/05/97/45e743b372f65a619f8d1eb2897efb74fb1b0ffddc731ad37e0aa187ec5c/graphene-2.1.8-py2.py3-none-any.whl (107kB)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.2.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (3.7.2)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Collecting pamqp==2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n",
            "Collecting multidict>=4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3.0.0,>=2.11.2; extra == \"all\"->fastapi[all]<1.0.0,>=0.38.1->reaction==20.2->-r neuro/requirements/requirements.txt (line 3)) (1.1.1)\n",
            "Collecting dnspython>=1.15.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/2d/ae9e172b4e5e72fa4b3cfc2517f38b602cc9ba31355f9669c502b4e9c458/dnspython-2.1.0-py3-none-any.whl (241kB)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn[standard]<0.14.0,>=0.12.0; extra == \"all\"->fastapi[all]<1.0.0,>=0.38.1->reaction==20.2->-r neuro/requirements/requirements.txt (line 3)) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n",
            "Collecting watchgod>=0.6; extra == \"standard\"\n",
            "  Downloading https://files.pythonhosted.org/packages/57/35/9a8da3fb6681e6eba662b2d249eea58cebf575e392271efac3344c172c5f/watchgod-0.7-py3-none-any.whl\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0; (sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\") and extra == \"standard\"\n",
            "  Downloading https://files.pythonhosted.org/packages/87/05/805df4850d9659efd69d00076269ae6adcb0e151d1922cff822ead2c432a/uvloop-0.15.2-cp37-cp37m-manylinux2010_x86_64.whl (3.8MB)\n",
            "Collecting httptools==0.1.*; (sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\") and extra == \"standard\"\n",
            "  Downloading https://files.pythonhosted.org/packages/44/52/295101ea5a60f9bee805a3ca422863600ba5cac4e2778ac7bd56efab1231/httptools-0.1.1-cp37-cp37m-manylinux1_x86_64.whl (217kB)\n",
            "Collecting python-dotenv>=0.13; extra == \"standard\"\n",
            "  Downloading https://files.pythonhosted.org/packages/32/2e/e4585559237787966aad0f8fd0fc31df1c4c9eb0e62de458c5b6cde954eb/python_dotenv-0.15.0-py2.py3-none-any.whl\n",
            "Collecting websockets==8.*; extra == \"standard\"\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n",
            "Collecting aniso8601<=7,>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/45/a4/b4fcadbdab46c2ec2d2f6f8b4ab3f64fd0040789ac7f065eba82119cd602/aniso8601-7.0.0-py2.py3-none-any.whl (42kB)\n",
            "Collecting graphql-relay<3,>=2\n",
            "  Downloading https://files.pythonhosted.org/packages/94/48/6022ea2e89cb936c3b933a0409c6e29bf8a68c050fe87d97f98aff6e5e9e/graphql_relay-2.0.1-py3-none-any.whl\n",
            "Collecting graphql-core<3,>=2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/11/71/d51beba3d8986fa6d8670ec7bcba989ad6e852d5ae99d95633e5dacc53e7/graphql_core-2.3.2-py2.py3-none-any.whl (252kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14.0->catalyst==20.10.1->-r neuro/requirements/requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: promise<3,>=2.2 in /usr/local/lib/python3.7/dist-packages (from graphql-relay<3,>=2->graphene<3.0.0,>=2.1.8; extra == \"all\"->fastapi[all]<1.0.0,>=0.38.1->reaction==20.2->-r neuro/requirements/requirements.txt (line 3)) (2.3)\n",
            "Collecting rx<2,>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/33/0f/5ef4ac78e2a538cc1b054eb86285fe0bf7a5dbaeaac2c584757c300515e2/Rx-1.6.1-py2.py3-none-any.whl (179kB)\n",
            "Building wheels for collected packages: python-multipart\n",
            "  Building wheel for python-multipart (setup.py): started\n",
            "  Building wheel for python-multipart (setup.py): finished with status 'done'\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-cp37-none-any.whl size=31671 sha256=bee895cb43dfa7167b7ff9d8cfc7e3c935f27310c10ef0187df869a6bf98e965\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/e6/66/14a866a3cbd6a0cabfbef91f7edf40aa03595ef6c88d6d1be4\n",
            "Successfully built python-multipart\n",
            "Installing collected packages: idna, requests, alchemy, smmap, gitdb, GitPython, deprecation, tensorboardX, catalyst, multidict, yarl, pamqp, aiormq, aio-pika, starlette, pydantic, async-exit-stack, orjson, dnspython, email-validator, aiofiles, python-multipart, ujson, h11, watchgod, uvloop, httptools, python-dotenv, websockets, uvicorn, aniso8601, rx, graphql-core, graphql-relay, graphene, fastapi, reaction, osfclient\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed GitPython-3.1.14 aio-pika-6.1.1 aiofiles-0.5.0 aiormq-2.9.1 alchemy-20.4 aniso8601-7.0.0 async-exit-stack-1.0.1 catalyst-20.10.1 deprecation-2.1.0 dnspython-2.1.0 email-validator-1.1.2 fastapi-0.63.0 gitdb-4.0.7 graphene-2.1.8 graphql-core-2.3.2 graphql-relay-2.0.1 h11-0.12.0 httptools-0.1.1 idna-2.8 multidict-5.1.0 orjson-3.5.1 osfclient-0.0.5 pamqp-2.3.0 pydantic-1.8.1 python-dotenv-0.15.0 python-multipart-0.0.5 reaction-20.2 requests-2.22.0 rx-1.6.1 smmap-4.0.0 starlette-0.13.6 tensorboardX-2.1 ujson-3.2.0 uvicorn-0.13.4 uvloop-0.15.2 watchgod-0.7 websockets-8.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'neuro'...\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "ERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\n",
            "ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1BnwldEP_L-",
        "outputId": "d2cbb19a-0174-43b6-d3ea-3f19120b9c46"
      },
      "source": [
        "from typing import Callable, List, Tuple\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import catalyst\n",
        "from catalyst import utils\n",
        "\n",
        "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
        "\n",
        "SEED = 42\n",
        "utils.set_global_seed(SEED)\n",
        "utils.prepare_cudnn(deterministic=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch: 1.8.0+cu101, catalyst: 20.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOGktFaFP_L_"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We'll be using the Mindboggle 101 dataset for a multiclass 3d segmentation task.\n",
        "The dataset can be downloaded off osf with the following command from osfclient after you register with osf.\n",
        "\n",
        "`osf -p 9ahyp clone .`\n",
        "\n",
        "Otherwise you can download it using a Catalyst utility `download-gdrive` which downloads a version from the Catalyst Google Drive\n",
        "\n",
        "`usage: download-gdrive {FILE_ID} {FILENAME}`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oupu01n3RJb5",
        "outputId": "27e66e30-bf48-4e94-c284-b64d94c2d769"
      },
      "source": [
        "cd neuro"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/neuro\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VlogHK7RGkJ"
      },
      "source": [
        "%%bash\n",
        "mkdir Mindboggle_data \n",
        "mkdir -p data/Mindboggle_101/\n",
        "osf -p 9ahyp clone Mindboggle_data/\n",
        "cp -r Mindboggle_data/osfstorage/Mindboggle101_volumes/ data/Mindboggle_101/\n",
        "find data/Mindboggle_101 -name '*.tar.gz'| xargs -i tar zxvf {} -C data/Mindboggle_101\n",
        "find data/Mindboggle_101 -name '*.tar.gz'| xargs -i rm {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCJ4_qnGP_MA"
      },
      "source": [
        "Run the prepare data script that limits the labels to the DKT human labels (60 labels).\n",
        "\n",
        "`usage: python ../neuro/scripts/prepare_data.py ../data/Mindboggle_101 {N_labels)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBFRPKyuP_MA"
      },
      "source": [
        "%%bash \n",
        "\n",
        "python neuro/scripts/prepare_data.py data/Mindboggle_101/ 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lucGpY0AP_MA"
      },
      "source": [
        "Import Catalyst and Torch utils for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHX508BDP_MA"
      },
      "source": [
        "import torch\n",
        "import collections\n",
        "\n",
        "from catalyst.contrib.utils.pandas import read_csv_data\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from catalyst.data import Augmentor, ReaderCompose\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from catalyst.dl import SupervisedRunner\n",
        "from catalyst.callbacks.logging import TensorboardLogger\n",
        "from catalyst.callbacks import SchedulerCallback, CheckpointCallback\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xVyRKWOhCIj"
      },
      "source": [
        "from torch.nn import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kviNcrfIP_MA"
      },
      "source": [
        "### **Create the relevant Dataloaders for the specified train, validation, and inference BrainDatasets.**\n",
        "\n",
        "BrainDatasets comprise of T1 scans + the prepared limited labels.\n",
        "\n",
        "Training/ Validation batches: Randomly Sampled NxNxN Subvolumes from a Normal Distribution across the Volume Space with their corresponding labels.\n",
        "\n",
        "Inference batches: Non-overlapping NxNxN Subvolumes across the existing volume space with their corresponding labels\n",
        "\n",
        "More detail can be found in brain_dataset.py and generator_coords.py  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeFGpQ4_Ufdw"
      },
      "source": [
        "cd training/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjVQEnItP_MA"
      },
      "source": [
        "from brain_dataset import BrainDataset\n",
        "from reader import NiftiReader_Image, NiftiReader_Mask\n",
        "from callbacks import CustomDiceCallback\n",
        "from model import UNet, MeshNet\n",
        "from custom_metrics import custom_dice_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSzbqRwhP_MA"
      },
      "source": [
        "open_fn = ReaderCompose(                                                                                                                                                                            \n",
        "    readers=[                                                                                                                                                                                       \n",
        "        NiftiReader_Image(input_key=\"images\", output_key=\"images\"),                                                                                                                                 \n",
        "        NiftiReader_Mask(input_key=\"nii_labels\", output_key=\"targets\"),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSWPJkTgP_MA"
      },
      "source": [
        "\n",
        "def get_loaders(\n",
        "    random_state: int,\n",
        "    volume_shape: List[int],\n",
        "    subvolume_shape: List[int],\n",
        "    in_csv_train: str = None,                                                                                                                                                                           \n",
        "    in_csv_valid: str = None,                                                                                                                                                                           \n",
        "    in_csv_infer: str = None,\n",
        "    batch_size: int = 16,\n",
        "    num_workers: int = 10,\n",
        ") -> dict:\n",
        "\n",
        "    df, df_train, df_valid, df_infer = read_csv_data(                                                                                                                                                   \n",
        "    in_csv_train=in_csv_train,                                                                                                                                                                      \n",
        "    in_csv_valid=in_csv_valid,                                                                                                                                                                      \n",
        "    in_csv_infer=in_csv_infer,                                                                                                                                                                      \n",
        "    ) \n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    train_dataset = BrainDataset(shared_dict={},                                                                                                                                                             \n",
        "                    list_data=df_train, list_shape=volume_shape, list_sub_shape=subvolume_shape,                                                                                                              \n",
        "                    open_fn=open_fn,                                                                                                         \n",
        "                    mode='train', input_key=\"images\",                                                                                                                                     \n",
        "                    output_key=\"targets\")\n",
        "    valid_dataset = BrainDataset(shared_dict={},                                                                                                                                                             \n",
        "                    list_data=df_valid, list_shape=volume_shape, list_sub_shape=subvolume_shape,                                                                                                              \n",
        "                    open_fn=open_fn,                                                                                                         \n",
        "                    mode='valid', input_key=\"images\",                                                                                                                                     \n",
        "                    output_key=\"targets\")\n",
        "    test_dataset = BrainDataset(shared_dict={},                                                                                                                                                             \n",
        "                    list_data=df_infer, list_shape=volume_shape, list_sub_shape=subvolume_shape,                                                                                                              \n",
        "                    open_fn=open_fn,                                                                                                         \n",
        "                    mode='infer', input_key=\"images\",                                                                                                                                     \n",
        "                    output_key=\"targets\")\n",
        "\n",
        "    train_random_sampler = RandomSampler(data_source=train_dataset,                                                                                                                                   \n",
        "                                          replacement=True,\n",
        "                                          num_samples=len(train_dataset) * 16)\n",
        "\n",
        "    valid_random_sampler = RandomSampler(data_source=valid_dataset,  \n",
        "                                          replacement=True,\n",
        "                                          num_samples=len(valid_dataset)*16)\n",
        "    \n",
        "    infer_random_sampler = SequentialSampler(data_source=test_dataset)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_random_sampler, \n",
        "                              num_workers=2, pin_memory=True)\n",
        "    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, sampler=valid_random_sampler, \n",
        "                              num_workers=2, pin_memory=True,drop_last=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, sampler=infer_random_sampler, \n",
        "                              num_workers=2, pin_memory=True,drop_last=True)\n",
        "    train_loaders = collections.OrderedDict()\n",
        "    infer_loaders = collections.OrderedDict()\n",
        "    train_loaders[\"train\"] = train_loader\n",
        "    train_loaders[\"valid\"] = valid_loader\n",
        "    infer_loaders['infer'] = test_loader\n",
        "\n",
        "    return train_loaders, infer_loaders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORmV32oeP_MA"
      },
      "source": [
        "train_loaders, infer_loaders = get_loaders(0, [256, 256, 256], [38, 38, 38], \n",
        "                      \"../data/dataset_train.csv\", \"../data/dataset_valid.csv\", \"../data/dataset_infer.csv\", )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Qav0lxP_MA"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "We'll train the model 1 epoch for demonstration although typically we train for 30 epochs.\n",
        "\n",
        "* Scheduler: Adam with a cosine annealing schedule starting at a learning rate of .01 \n",
        "* Batch Metric: DICE\n",
        "* Loss: CrossEntropyLoss\n",
        "* Logger: Tensorboard\n",
        "* CheckpointerCallback\n",
        "\n",
        "For training and validation we sample the volume with subvolumes specified in our Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40l1IuotXREo"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFnFWvyZXt5l"
      },
      "source": [
        "class CustomRunner(catalyst.dl.Runner):\n",
        "\n",
        "    def predict_batch(self, batch):\n",
        "        # model inference step\n",
        "        return self.model(batch['images'].to(self.device)), batch['coords']\n",
        "\n",
        "    def _handle_batch(self, batch):\n",
        "        # model train/valid step\n",
        "        x, y = batch['images'], batch['targets']\n",
        "        y_hat = self.model(x)\n",
        "\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.batch_metrics.update({\"loss\": loss, \"dice\": custom_dice_metric(y_hat.float(), y, num_classes=60, activation='Softmax')})\n",
        "\n",
        "        if self.is_train_loader:\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ9dzP86dAmx"
      },
      "source": [
        "n_classes = 60\n",
        "torch.backends.cudnn.deterministic = False\n",
        "meshnet = MeshNet(n_channels=1, n_classes=n_classes)\n",
        "\n",
        "logdir = \"logs/meshnet\"\n",
        "\n",
        "optimizer = torch.optim.Adam(meshnet.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=30)\n",
        "\n",
        "runner = CustomRunner()\n",
        "runner.train(model=meshnet, optimizer=optimizer, loaders=train_loaders, num_epochs=1, scheduler=scheduler, \n",
        "             callbacks=[TensorboardLogger(), CheckpointCallback()], logdir=logdir, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al25kqCRV4En"
      },
      "source": [
        "def voxel_majority_predict_from_subvolumes(loader, volume_shape, n_classes):\n",
        "  segmentations = {}\n",
        "  for subject in range(loader.dataset.subjects - 1):\n",
        "    segmentations[subject] = torch.zeros(tuple(np.insert(volume_shape, 0, n_classes)), dtype=torch.uint8)\n",
        "\n",
        "  for inference in runner.predict_loader(loader=loader):\n",
        "    subj_id = loader.dataset.subjects // len(loader.dataset.coords)\n",
        "    coords = inference[1]\n",
        "    predicted = inference[0].cpu()\n",
        "    for j in range(predicted.shape[0]):\n",
        "      c_j = coords[j]\n",
        "      for c in range(n_classes):\n",
        "        segmentations[subj_id][c, c_j[0, 0]:c_j[0, 1], \n",
        "                              c_j[1, 0]:c_j[1, 1], \n",
        "                              c_j[2, 0]:c_j[2, 1]] += (predicted[j] == c)\n",
        "\n",
        "  for i in segmentations.keys():\n",
        "    segmentations[i] = torch.max(segmentations[i], 0)[1]\n",
        "  return segmentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ISsa72vSgTg"
      },
      "source": [
        "inference[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCrUca9LRyF9"
      },
      "source": [
        "for i in SequentialSampler(infer_loader.dataset): \n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYLpKZq6RS1a"
      },
      "source": [
        "infer_loader.sampler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TdoZPOGVbaQ"
      },
      "source": [
        "from torch.nn.functional import log_softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tgmntvkJTUY"
      },
      "source": [
        "segmentations = {}\n",
        "for subject in range(infer_loader.dataset.subjects):\n",
        "  segmentations[subject] = torch.zeros(tuple(np.insert([256, 256, 256], 0, 60)), dtype=torch.uint8)\n",
        "\n",
        "for inference in runner.predict_loader(loader=infer_loader):\n",
        "  subj_id = infer_loader.dataset.subjects // len(infer_loader.dataset.coords)\n",
        "  coords = inference[1]\n",
        "  print(coords)\n",
        "  _, predicted = torch.max(log_softmax(inference[0].cpu(), dim=1), 1)\n",
        "  for j in range(predicted.shape[0]):\n",
        "    c_j = coords[j][0]\n",
        "    for c in range(n_classes):\n",
        "      segmentations[subj_id][c, c_j[0, 0]:c_j[0, 1], \n",
        "                            c_j[1, 0]:c_j[1, 1], \n",
        "                            c_j[2, 0]:c_j[2, 1]] += (predicted[j] == c)\n",
        "\n",
        "for i in segmentations.keys():\n",
        "  segmentations[i] = torch.max(segmentations[i], 0)[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRbkX-F9hcT1"
      },
      "source": [
        "n_classes = 60\n",
        "infer_loader = infer_loaders['infer']\n",
        "\n",
        "segmentations = voxel_majority_predict_from_subvolumes(infer_loader, [256, 256, 256], n_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCoyo9a6HQVf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}