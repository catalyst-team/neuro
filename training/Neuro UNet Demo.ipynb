{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuro UNet/ MeshnetTutorial\n",
    "\n",
    "Authors: [Kevin Wang] (), [Alex Fedorov] (), [Sergey Kolesnikov](https://github.com/Scitator)\n",
    "\n",
    "[![Catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)](https://github.com/catalyst-team/catalyst)\n",
    "\n",
    "### Colab setup\n",
    "\n",
    "First of all, do not forget to change the runtime type to GPU. <br/>\n",
    "To do so click `Runtime` -> `Change runtime type` -> Select `\\\"Python 3\\\"` and `\\\"GPU\\\"` -> click `Save`. <br/>\n",
    "After that you can click `Runtime` -> `Run all` and watch the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Download and install the latest versions of catalyst and other libraries required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import catalyst\n",
    "from catalyst import utils\n",
    "\n",
    "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
    "\n",
    "SEED = 42\n",
    "utils.set_global_seed(SEED)\n",
    "utils.prepare_cudnn(deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We'll be using the Mindboggle 101 dataset for a multiclass 3d segmentation task.\n",
    "The dataset can be downloaded off osf with the following command from osfclient after you register with osf.\n",
    "\n",
    "`osf -p 9ahyp clone .`\n",
    "\n",
    "Otherwise you can download it using a Catalyst utility `download-gdrive` which downloads a version from the Catalyst Google Drive\n",
    "\n",
    "`usage: download-gdrive {FILE_ID} {FILENAME}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir Mindboggle_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# todo: make download-gdrive here\n",
    "osf -p 9ahyp clone Mindboggle_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and extract volumes to the following location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -r Mindboggle_data/osfstorage/Mindboggle101_volumes/ ../data/Mindboggle_data/\n",
    "find data/Mindboggle_101 -name '*.tar.gz'| xargs -i tar zxvf {} -C data/Mindboggle_101\n",
    "find data/Mindboggle_101 -name '*.tar.gz'| xargs -i rm {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prepare data script that limits the labels to 30 labels.\n",
    "\n",
    "`usage: python ../neuro/scripts/prepare_data.py ../data/Mindboggle_101 {N_labels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "python ../neuro/scripts/prepare_data.py ../data/Mindboggle_101/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Catalyst and Torch utils for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "\n",
    "from multiprocessing import Manager\n",
    "from catalyst.contrib.utils.pandas import read_csv_data\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from catalyst.data import Augmentor, ReaderCompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import a BrainDataSet, which reads T1 scans and labels and samples either random patches of 38x38x38 samples from them or nonoverlapping patches of 38x38x38 for validation.  More detail can be found in brain_dataset.py and generator_coords.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brain_dataset import BrainDataset\n",
    "from reader import NiftiReader_Image, NiftiReader_Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transforms for the BrainDataset here are simple. \n",
    "\n",
    "Convert the T1 scans from numpy arrays to PyTorch floats and convert the corresponding labels to whatever default Torch array exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(stage: str = None, mode: str = None):\n",
    "    if mode == \"train\":                                                                                                                                                                                 \n",
    "        Augmentor1 = Augmentor(                                                                                                                                                                         \n",
    "            dict_key=\"images\",                                                                                                                                                                          \n",
    "            augment_fn=lambda x: torch.from_numpy(x).float(),                                                                                                                                           \n",
    "        )                                                                                                                                                                                               \n",
    "        Augmentor2 = Augmentor(                                                                                                                                                                         \n",
    "            dict_key=\"targets\", augment_fn=lambda x: torch.from_numpy(x)                                                                                                                                \n",
    "        )                                                                                                                                                                                               \n",
    "        return transforms.Compose([Augmentor1, Augmentor2])                                                                                                                                             \n",
    "    elif mode == \"valid\":                                                                                                                                                                               \n",
    "        Augmentor1 = Augmentor(                                                                                                                                                                         \n",
    "            dict_key=\"images\",                                                                                                                                                                          \n",
    "            augment_fn=lambda x: torch.from_numpy(x).float(),                                                                                                                                           \n",
    "        )                                                                                                                                                                                               \n",
    "        Augmentor2 = Augmentor(                                                                                                                                                                         \n",
    "            dict_key=\"targets\", augment_fn=lambda x: torch.from_numpy(x)                                                                                                                                \n",
    "        )                                                                                                                                                                                               \n",
    "        return transforms.Compose([Augmentor1, Augmentor2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_fn = ReaderCompose(                                                                                                                                                                            \n",
    "    readers=[                                                                                                                                                                                       \n",
    "        NiftiReader_Image(input_key=\"images\", output_key=\"images\"),                                                                                                                                 \n",
    "        NiftiReader_Mask(input_key=\"nii_labels\", output_key=\"targets\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "    volume_shape: List[int],\n",
    "    subvolume_shape: List[int],\n",
    "    in_csv_train: str = None,\n",
    "    in_csv_valid: str = None,\n",
    "    in_csv_infer: str = None,\n",
    "    batch_size: int = 16,\n",
    "    num_workers: int = 10,\n",
    ") -> dict:\n",
    "\n",
    "    df, df_train, df_valid, df_infer = read_csv_data(\n",
    "        in_csv_train=in_csv_train,\n",
    "        in_csv_valid=in_csv_valid,\n",
    "        in_csv_infer=in_csv_infer,\n",
    "    )\n",
    "\n",
    "    train_dataset = BrainDataset(\n",
    "        shared_dict={},\n",
    "        list_data=df_train,\n",
    "        list_shape=volume_shape,\n",
    "        list_sub_shape=subvolume_shape,\n",
    "        open_fn=open_fn,\n",
    "        dict_transform=get_transforms(None, mode=\"train\"),\n",
    "        n_samples=100,\n",
    "        mode=\"train\",\n",
    "        input_key=\"images\",\n",
    "        output_key=\"targets\",\n",
    "    )\n",
    "    valid_dataset = BrainDataset(\n",
    "        shared_dict={},\n",
    "        list_data=df_valid,\n",
    "        list_shape=volume_shape,\n",
    "        list_sub_shape=subvolume_shape,\n",
    "        open_fn=open_fn,\n",
    "        dict_transform=get_transforms(None, mode=\"valid\"),\n",
    "        n_samples=100,\n",
    "        mode=\"valid\",\n",
    "        input_key=\"images\",\n",
    "        output_key=\"targets\",\n",
    "    )\n",
    "    # test_dataset = BrainDataset(\n",
    "    #     shared_dict={},\n",
    "    #     list_data=df_infer,\n",
    "    #     list_shape=volume_shape,\n",
    "    #     list_sub_shape=subvolume_shape,\n",
    "    #     open_fn=open_fn,\n",
    "    #     dict_transform=get_transforms(None, mode=\"valid\"),\n",
    "    #     n_samples=100,\n",
    "    #     mode=\"valid\",\n",
    "    #     input_key=\"images\",\n",
    "    #     output_key=\"targets\",\n",
    "    # )\n",
    "\n",
    "    train_random_sampler = RandomSampler(\n",
    "        data_source=train_dataset, replacement=True, num_samples=80 * 128\n",
    "    )\n",
    "\n",
    "    valid_random_sampler = RandomSampler(\n",
    "        data_source=valid_dataset, replacement=True, num_samples=20 * 216\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_random_sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=valid_random_sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    loaders = collections.OrderedDict()\n",
    "    loaders[\"train\"] = train_loader\n",
    "    loaders[\"valid\"] = valid_loader\n",
    "\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = get_loaders(\n",
    "    0,\n",
    "    [256, 256, 256],\n",
    "    [38, 38, 38],\n",
    "    \"../data/dataset_train.csv\",\n",
    "    \"../data/dataset_valid.csv\",\n",
    "    \"../data/dataset_infer.csv\",\n",
    ")\n",
    "train_dataloader = loaders[\"train\"]\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We'll be using the UNet defined in the model.py file for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import UNet\n",
    "\n",
    "unet = UNet(n_channels=1, n_classes=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "We'll train the model 30 epochs\n",
    "\n",
    "An Adam Optimizer with a cosine annealing schedule starting at a learning rate of .01 is used for this experiment.\n",
    "\n",
    "CrossEntropyLoss is the criterion/ loss function be minimized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.callbacks.logging import TensorboardLogger\n",
    "from catalyst.callbacks import SchedulerCallback, CheckpointCallback\n",
    "from custom_metrics import CustomDiceCallback\n",
    "\n",
    "num_epochs = 30\n",
    "logdir = \"logs/unet\"\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "runner = SupervisedRunner(\n",
    "    input_key=\"images\", input_target_key=\"labels\", output_key=\"logits\"\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    TensorboardLogger(),\n",
    "    SchedulerCallback(reduced_metric=\"loss\"),\n",
    "    CustomDiceCallback(),\n",
    "    CheckpointCallback(),\n",
    "]\n",
    "\n",
    "runner.train(\n",
    "    model=unet,\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    callbacks=callbacks,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}